{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "\n",
    "number_classes = 7 \n",
    "rows=48\n",
    "cols=48\n",
    "batch_size = 16\n",
    "\n",
    "train_data_direction = './dataset/train'\n",
    "test_data_direction = './dataset/test'\n",
    "\n",
    "#Using Data Augmentation\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                  rotation_range=30,\n",
    "                                  shear_range=0.3,\n",
    "                                  zoom_range=0.3,\n",
    "                                  width_shift_range=0.4,\n",
    "                                  height_shift_range=0.4,\n",
    "                                  horizontal_flip=True,\n",
    "                                  fill_mode='nearest')\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_direction,\n",
    "        color_mode = 'grayscale',\n",
    "        target_size = (rows , cols),\n",
    "        batch_size = batch_size,\n",
    "        class_mode = 'categorical',\n",
    "        shuffle = True)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        test_data_direction,\n",
    "        color_mode = 'grayscale',\n",
    "        target_size = (rows , cols),\n",
    "        batch_size = batch_size,\n",
    "        class_mode = 'categorical',\n",
    "        shuffle = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28709"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import ELU\n",
    "from tensorflow.keras.layers import Activation, Flatten, Dropout, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 48, 48, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 48, 48, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 48, 48, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 48, 48, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 24, 24, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 24, 24, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 24, 24, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 12, 12, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 12, 12, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 6, 6, 256)         295168    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 6, 6, 256)         590080    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                147520    \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 7)                 455       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 7)                 0         \n",
      "=================================================================\n",
      "Total params: 1,328,167\n",
      "Trainable params: 1,325,991\n",
      "Non-trainable params: 2,176\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3,3), padding='same', kernel_initializer='he_normal', input_shape=(rows,cols,1)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(32, (3,3), padding='same', kernel_initializer='he_normal', input_shape=(rows,cols,1)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "model.add(Conv2D(64, (3,3), padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(64, (3,3), padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "model.add(Conv2D(128, (3,3), padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(128, (3,3), padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "model.add(Conv2D(256, (3,3), padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(256, (3,3), padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(64, kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "#Softmax Activation in Output\n",
    "model.add(Dense(number_classes, kernel_initializer='he_normal'))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "1793/1794 [============================>.] - ETA: 0s - loss: 1.2020 - accuracy: 0.5579\n",
      "Epoch 00001: val_loss improved from inf to 0.97827, saving model to ./checkpoint\\emotion_model.h5\n",
      "1794/1794 [==============================] - 21s 12ms/step - loss: 1.2020 - accuracy: 0.5579 - val_loss: 0.9783 - val_accuracy: 0.6309\n",
      "Epoch 2/40\n",
      "1790/1794 [============================>.] - ETA: 0s - loss: 1.2060 - accuracy: 0.5560\n",
      "Epoch 00002: val_loss improved from 0.97827 to 0.97087, saving model to ./checkpoint\\emotion_model.h5\n",
      "1794/1794 [==============================] - 20s 11ms/step - loss: 1.2059 - accuracy: 0.5560 - val_loss: 0.9709 - val_accuracy: 0.6369\n",
      "Epoch 3/40\n",
      "1790/1794 [============================>.] - ETA: 0s - loss: 1.2135 - accuracy: 0.5541\n",
      "Epoch 00003: val_loss did not improve from 0.97087\n",
      "1794/1794 [==============================] - 20s 11ms/step - loss: 1.2132 - accuracy: 0.5542 - val_loss: 0.9818 - val_accuracy: 0.6334\n",
      "Epoch 4/40\n",
      "1791/1794 [============================>.] - ETA: 0s - loss: 1.2106 - accuracy: 0.5528\n",
      "Epoch 00004: val_loss did not improve from 0.97087\n",
      "1794/1794 [==============================] - 21s 11ms/step - loss: 1.2103 - accuracy: 0.5529 - val_loss: 0.9801 - val_accuracy: 0.6277\n",
      "Epoch 5/40\n",
      "1790/1794 [============================>.] - ETA: 0s - loss: 1.2083 - accuracy: 0.5540\n",
      "Epoch 00005: val_loss did not improve from 0.97087\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "1794/1794 [==============================] - 20s 11ms/step - loss: 1.2078 - accuracy: 0.5542 - val_loss: 0.9841 - val_accuracy: 0.6327\n",
      "Epoch 6/40\n",
      "1793/1794 [============================>.] - ETA: 0s - loss: 1.1860 - accuracy: 0.5638\n",
      "Epoch 00006: val_loss improved from 0.97087 to 0.96068, saving model to ./checkpoint\\emotion_model.h5\n",
      "1794/1794 [==============================] - 20s 11ms/step - loss: 1.1862 - accuracy: 0.5638 - val_loss: 0.9607 - val_accuracy: 0.6415\n",
      "Epoch 7/40\n",
      "1794/1794 [==============================] - ETA: 0s - loss: 1.1818 - accuracy: 0.5646\n",
      "Epoch 00007: val_loss improved from 0.96068 to 0.95986, saving model to ./checkpoint\\emotion_model.h5\n",
      "1794/1794 [==============================] - 21s 11ms/step - loss: 1.1818 - accuracy: 0.5646 - val_loss: 0.9599 - val_accuracy: 0.6408\n",
      "Epoch 8/40\n",
      "1793/1794 [============================>.] - ETA: 0s - loss: 1.1771 - accuracy: 0.5667\n",
      "Epoch 00008: val_loss improved from 0.95986 to 0.95827, saving model to ./checkpoint\\emotion_model.h5\n",
      "1794/1794 [==============================] - 21s 11ms/step - loss: 1.1770 - accuracy: 0.5667 - val_loss: 0.9583 - val_accuracy: 0.6408\n",
      "Epoch 9/40\n",
      "1793/1794 [============================>.] - ETA: 0s - loss: 1.1693 - accuracy: 0.5688\n",
      "Epoch 00009: val_loss improved from 0.95827 to 0.95770, saving model to ./checkpoint\\emotion_model.h5\n",
      "1794/1794 [==============================] - 20s 11ms/step - loss: 1.1692 - accuracy: 0.5688 - val_loss: 0.9577 - val_accuracy: 0.6402\n",
      "Epoch 10/40\n",
      "1793/1794 [============================>.] - ETA: 0s - loss: 1.1665 - accuracy: 0.5702\n",
      "Epoch 00010: val_loss improved from 0.95770 to 0.95535, saving model to ./checkpoint\\emotion_model.h5\n",
      "1794/1794 [==============================] - 20s 11ms/step - loss: 1.1666 - accuracy: 0.5701 - val_loss: 0.9554 - val_accuracy: 0.6388\n",
      "Epoch 11/40\n",
      "1790/1794 [============================>.] - ETA: 0s - loss: 1.1701 - accuracy: 0.5697\n",
      "Epoch 00011: val_loss did not improve from 0.95535\n",
      "1794/1794 [==============================] - 20s 11ms/step - loss: 1.1702 - accuracy: 0.5697 - val_loss: 0.9600 - val_accuracy: 0.6378\n",
      "Epoch 12/40\n",
      "1793/1794 [============================>.] - ETA: 0s - loss: 1.1579 - accuracy: 0.5744\n",
      "Epoch 00012: val_loss improved from 0.95535 to 0.95439, saving model to ./checkpoint\\emotion_model.h5\n",
      "1794/1794 [==============================] - 20s 11ms/step - loss: 1.1579 - accuracy: 0.5745 - val_loss: 0.9544 - val_accuracy: 0.6430\n",
      "Epoch 13/40\n",
      "1794/1794 [==============================] - ETA: 0s - loss: 1.1624 - accuracy: 0.5733\n",
      "Epoch 00013: val_loss did not improve from 0.95439\n",
      "1794/1794 [==============================] - 20s 11ms/step - loss: 1.1624 - accuracy: 0.5733 - val_loss: 0.9546 - val_accuracy: 0.6437\n",
      "Epoch 14/40\n",
      "1789/1794 [============================>.] - ETA: 0s - loss: 1.1571 - accuracy: 0.5757\n",
      "Epoch 00014: val_loss improved from 0.95439 to 0.95003, saving model to ./checkpoint\\emotion_model.h5\n",
      "1794/1794 [==============================] - 20s 11ms/step - loss: 1.1569 - accuracy: 0.5759 - val_loss: 0.9500 - val_accuracy: 0.6444\n",
      "Epoch 15/40\n",
      "1794/1794 [==============================] - ETA: 0s - loss: 1.1641 - accuracy: 0.5699\n",
      "Epoch 00015: val_loss did not improve from 0.95003\n",
      "1794/1794 [==============================] - 21s 12ms/step - loss: 1.1641 - accuracy: 0.5699 - val_loss: 0.9597 - val_accuracy: 0.6426\n",
      "Epoch 16/40\n",
      "1793/1794 [============================>.] - ETA: 0s - loss: 1.1583 - accuracy: 0.5725\n",
      "Epoch 00016: val_loss did not improve from 0.95003\n",
      "1794/1794 [==============================] - 21s 12ms/step - loss: 1.1582 - accuracy: 0.5725 - val_loss: 0.9556 - val_accuracy: 0.6451\n",
      "Epoch 17/40\n",
      "1792/1794 [============================>.] - ETA: 0s - loss: 1.1539 - accuracy: 0.5738\n",
      "Epoch 00017: val_loss did not improve from 0.95003\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "1794/1794 [==============================] - 20s 11ms/step - loss: 1.1540 - accuracy: 0.5737 - val_loss: 0.9508 - val_accuracy: 0.6459\n",
      "Epoch 18/40\n",
      "1791/1794 [============================>.] - ETA: 0s - loss: 1.1586 - accuracy: 0.5771\n",
      "Epoch 00018: val_loss improved from 0.95003 to 0.94785, saving model to ./checkpoint\\emotion_model.h5\n",
      "1794/1794 [==============================] - 21s 11ms/step - loss: 1.1587 - accuracy: 0.5773 - val_loss: 0.9479 - val_accuracy: 0.6469\n",
      "Epoch 19/40\n",
      "1792/1794 [============================>.] - ETA: 0s - loss: 1.1528 - accuracy: 0.5765\n",
      "Epoch 00019: val_loss improved from 0.94785 to 0.94756, saving model to ./checkpoint\\emotion_model.h5\n",
      "1794/1794 [==============================] - 22s 12ms/step - loss: 1.1528 - accuracy: 0.5764 - val_loss: 0.9476 - val_accuracy: 0.6476\n",
      "Epoch 20/40\n",
      "1793/1794 [============================>.] - ETA: 0s - loss: 1.1509 - accuracy: 0.5760\n",
      "Epoch 00020: val_loss improved from 0.94756 to 0.94744, saving model to ./checkpoint\\emotion_model.h5\n",
      "1794/1794 [==============================] - 24s 14ms/step - loss: 1.1507 - accuracy: 0.5761 - val_loss: 0.9474 - val_accuracy: 0.6461\n",
      "Epoch 21/40\n",
      "1791/1794 [============================>.] - ETA: 0s - loss: 1.1590 - accuracy: 0.5735\n",
      "Epoch 00021: val_loss did not improve from 0.94744\n",
      "1794/1794 [==============================] - 21s 12ms/step - loss: 1.1593 - accuracy: 0.5734 - val_loss: 0.9500 - val_accuracy: 0.6452\n",
      "Epoch 22/40\n",
      "1791/1794 [============================>.] - ETA: 0s - loss: 1.1497 - accuracy: 0.5779\n",
      "Epoch 00022: val_loss did not improve from 0.94744\n",
      "1794/1794 [==============================] - 20s 11ms/step - loss: 1.1500 - accuracy: 0.5781 - val_loss: 0.9532 - val_accuracy: 0.6445\n",
      "Epoch 23/40\n",
      "1789/1794 [============================>.] - ETA: 0s - loss: 1.1443 - accuracy: 0.5798\n",
      "Epoch 00023: val_loss improved from 0.94744 to 0.94501, saving model to ./checkpoint\\emotion_model.h5\n",
      "1794/1794 [==============================] - 20s 11ms/step - loss: 1.1443 - accuracy: 0.5795 - val_loss: 0.9450 - val_accuracy: 0.6489\n",
      "Epoch 24/40\n",
      "1794/1794 [==============================] - ETA: 0s - loss: 1.1524 - accuracy: 0.5774\n",
      "Epoch 00024: val_loss did not improve from 0.94501\n",
      "1794/1794 [==============================] - 20s 11ms/step - loss: 1.1524 - accuracy: 0.5774 - val_loss: 0.9490 - val_accuracy: 0.6484\n",
      "Epoch 25/40\n",
      "1791/1794 [============================>.] - ETA: 0s - loss: 1.1553 - accuracy: 0.5770\n",
      "Epoch 00025: val_loss did not improve from 0.94501\n",
      "1794/1794 [==============================] - 20s 11ms/step - loss: 1.1551 - accuracy: 0.5772 - val_loss: 0.9484 - val_accuracy: 0.6463\n",
      "Epoch 26/40\n",
      "1791/1794 [============================>.] - ETA: 0s - loss: 1.1359 - accuracy: 0.5841\n",
      "Epoch 00026: val_loss did not improve from 0.94501\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "1794/1794 [==============================] - 20s 11ms/step - loss: 1.1358 - accuracy: 0.5841 - val_loss: 0.9479 - val_accuracy: 0.6473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/40\n",
      "1792/1794 [============================>.] - ETA: 0s - loss: 1.1442 - accuracy: 0.5788\n",
      "Epoch 00027: val_loss did not improve from 0.94501\n",
      "1794/1794 [==============================] - 20s 11ms/step - loss: 1.1440 - accuracy: 0.5790 - val_loss: 0.9479 - val_accuracy: 0.6475\n",
      "Epoch 28/40\n",
      "1789/1794 [============================>.] - ETA: 0s - loss: 1.1477 - accuracy: 0.5815\n",
      "Epoch 00028: val_loss did not improve from 0.94501\n",
      "1794/1794 [==============================] - 20s 11ms/step - loss: 1.1477 - accuracy: 0.5814 - val_loss: 0.9465 - val_accuracy: 0.6484\n",
      "Epoch 29/40\n",
      "1794/1794 [==============================] - ETA: 0s - loss: 1.1439 - accuracy: 0.5813\n",
      "Epoch 00029: val_loss did not improve from 0.94501\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "1794/1794 [==============================] - 20s 11ms/step - loss: 1.1439 - accuracy: 0.5813 - val_loss: 0.9451 - val_accuracy: 0.6487\n",
      "Epoch 30/40\n",
      "1789/1794 [============================>.] - ETA: 0s - loss: 1.1520 - accuracy: 0.5771\n",
      "Epoch 00030: val_loss improved from 0.94501 to 0.94493, saving model to ./checkpoint\\emotion_model.h5\n",
      "1794/1794 [==============================] - 21s 11ms/step - loss: 1.1520 - accuracy: 0.5771 - val_loss: 0.9449 - val_accuracy: 0.6479\n",
      "Epoch 31/40\n",
      "1793/1794 [============================>.] - ETA: 0s - loss: 1.1505 - accuracy: 0.5813\n",
      "Epoch 00031: val_loss did not improve from 0.94493\n",
      "1794/1794 [==============================] - 21s 11ms/step - loss: 1.1504 - accuracy: 0.5813 - val_loss: 0.9463 - val_accuracy: 0.6487\n",
      "Epoch 32/40\n",
      "1793/1794 [============================>.] - ETA: 0s - loss: 1.1499 - accuracy: 0.5768\n",
      "Epoch 00032: val_loss improved from 0.94493 to 0.94484, saving model to ./checkpoint\\emotion_model.h5\n",
      "1794/1794 [==============================] - 20s 11ms/step - loss: 1.1498 - accuracy: 0.5769 - val_loss: 0.9448 - val_accuracy: 0.6498\n",
      "Epoch 33/40\n",
      "1793/1794 [============================>.] - ETA: 0s - loss: 1.1413 - accuracy: 0.5802\n",
      "Epoch 00033: val_loss did not improve from 0.94484\n",
      "1794/1794 [==============================] - 20s 11ms/step - loss: 1.1413 - accuracy: 0.5802 - val_loss: 0.9476 - val_accuracy: 0.6479\n",
      "Epoch 34/40\n",
      "1793/1794 [============================>.] - ETA: 0s - loss: 1.1465 - accuracy: 0.5797\n",
      "Epoch 00034: val_loss did not improve from 0.94484\n",
      "1794/1794 [==============================] - 22s 12ms/step - loss: 1.1466 - accuracy: 0.5795 - val_loss: 0.9481 - val_accuracy: 0.6477\n",
      "Epoch 35/40\n",
      "1790/1794 [============================>.] - ETA: 0s - loss: 1.1548 - accuracy: 0.5773\n",
      "Epoch 00035: val_loss did not improve from 0.94484\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "1794/1794 [==============================] - 22s 12ms/step - loss: 1.1547 - accuracy: 0.5773 - val_loss: 0.9485 - val_accuracy: 0.6484\n",
      "Epoch 36/40\n",
      "1791/1794 [============================>.] - ETA: 0s - loss: 1.1545 - accuracy: 0.5759\n",
      "Epoch 00036: val_loss did not improve from 0.94484\n",
      "1794/1794 [==============================] - 22s 12ms/step - loss: 1.1546 - accuracy: 0.5759 - val_loss: 0.9456 - val_accuracy: 0.6479\n",
      "Epoch 37/40\n",
      "1791/1794 [============================>.] - ETA: 0s - loss: 1.1527 - accuracy: 0.5738\n",
      "Epoch 00037: val_loss did not improve from 0.94484\n",
      "1794/1794 [==============================] - 21s 12ms/step - loss: 1.1534 - accuracy: 0.5735 - val_loss: 0.9475 - val_accuracy: 0.6480\n",
      "Epoch 38/40\n",
      "1792/1794 [============================>.] - ETA: 0s - loss: 1.1462 - accuracy: 0.5799 ETA: 0s - loss: 1.148\n",
      "Epoch 00038: val_loss did not improve from 0.94484\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
      "1794/1794 [==============================] - 21s 12ms/step - loss: 1.1459 - accuracy: 0.5800 - val_loss: 0.9496 - val_accuracy: 0.6437\n",
      "Epoch 39/40\n",
      "1793/1794 [============================>.] - ETA: 0s - loss: 1.1475 - accuracy: 0.5798\n",
      "Epoch 00039: val_loss did not improve from 0.94484\n",
      "1794/1794 [==============================] - 21s 12ms/step - loss: 1.1474 - accuracy: 0.5799 - val_loss: 0.9480 - val_accuracy: 0.6475\n",
      "Epoch 40/40\n",
      "1792/1794 [============================>.] - ETA: 0s - loss: 1.1513 - accuracy: 0.5776\n",
      "Epoch 00040: val_loss improved from 0.94484 to 0.94457, saving model to ./checkpoint\\emotion_model.h5\n",
      "1794/1794 [==============================] - 21s 12ms/step - loss: 1.1513 - accuracy: 0.5776 - val_loss: 0.9446 - val_accuracy: 0.6480\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop, SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"./checkpoint/emotion_model.h5\",\n",
    "                            monitor=\"val_loss\",\n",
    "                            mode=\"min\",\n",
    "                            save_best_only=True,\n",
    "                            verbose=1)\n",
    "\n",
    "earlystop = EarlyStopping(monitor = 'val_loss',\n",
    "                         min_delta = 0,\n",
    "                         patience = 10,\n",
    "                         verbose=1,\n",
    "                         restore_best_weights = True)\n",
    "\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor=0.2, patience=3, verbose=1, min_delta=0.0001)\n",
    "\n",
    "#Create callback list\n",
    "callbacks = [earlystop, checkpoint, reduce_lr]\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "             optimizer = Adam(lr=0.001),\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "\n",
    "total_train_samples = train_generator.samples\n",
    "total_test_samples = test_generator.samples\n",
    "epochs = 40\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "            train_generator,\n",
    "            steps_per_epoch=total_train_samples // batch_size,\n",
    "            epochs = epochs,\n",
    "            callbacks = callbacks,\n",
    "            validation_data = test_generator,\n",
    "            validation_steps = total_test_samples // batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenCV Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "classifier = load_model('./checkpoint/emotion_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7178 images belonging to 7 classes.\n",
      "{0: 'angry', 1: 'disgust', 2: 'fear', 3: 'happy', 4: 'neutral', 5: 'sad', 6: 'surprise'}\n"
     ]
    }
   ],
   "source": [
    "test_generator = test_datagen.flow_from_directory(\n",
    "        test_data_direction,\n",
    "        color_mode = 'grayscale',\n",
    "        target_size = (rows , cols),\n",
    "        batch_size = batch_size,\n",
    "        class_mode = 'categorical',\n",
    "        shuffle = True)\n",
    "\n",
    "class_labels = test_generator.class_indices\n",
    "class_labels = {v : k for k,v in class_labels.items()}\n",
    "classes = list(class_labels.values())\n",
    "print(class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "\n",
    "face_classifier = cv2.CascadeClassifier('./haarcascades/haarcascade_frontalface_default.xml')\n",
    "\n",
    "def face_detector(img):\n",
    "    \n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
    "    if faces is ():\n",
    "        return (0,0,0,0), np.zeros((48,48), np.uint8), img\n",
    "    \n",
    "    for(x,y,w,h) in faces:\n",
    "        x = x-50\n",
    "        w = w+50\n",
    "        y = y-50\n",
    "        h = h+50\n",
    "        cv2.rectangle(img, (x,y), (x+w,y+h) , (255,0,0), 2)\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        \n",
    "    try:\n",
    "        roi_gray = cv2.resize(roi_gray, (48,48), interpolation = cv2.INTER_AREA)\n",
    "    except:\n",
    "        return (x,w,y,h), np.zeros((48,48), np.uint8), img\n",
    "    return (x,w,y,h), roi_gray, img\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    rect, face, image = face_detector(frame)\n",
    "    if np.sum([face]) != 0.0:\n",
    "        roi = face.astype(\"float\") / 255.0\n",
    "        roi = img_to_array(roi)\n",
    "        roi = np.expand_dims(roi, axis=0)\n",
    "        \n",
    "        preds = classifier.predict(roi)[0]\n",
    "        label = class_labels[preds.argmax()]\n",
    "        label_position = (rect[0] + int((rect[1]/2)), rect[2] + 25)\n",
    "        cv2.putText(image, label, label_position, cv2.FONT_HERSHEY_SIMPLEX, 2, (0,255,0), 3)\n",
    "    else:\n",
    "        cv2.putText(image, \"No Face found\", (20,60), cv2.FONT_HERSHEY_SIMPLEX, 2, (0,255,0), 3)\n",
    "        \n",
    "    cv2.imshow('Emotion Detector', image)\n",
    "    if cv2.waitKey(1) == 13:\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
